---
title: 'Project 2: Modeling, Testing, and Predicting'
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Sydney Pham 
#Introduction 
####For this project, I chose a dataset exploring various factors that may predict the onset of heart disease. I obtained the dataset from the website Kaggle, but the data itself was compiled from various doctors from around the world including Switzerland, Budapest, Cleveland and Long Beach. There are a total of 14 variables in the dataset, but I decided to keep the following eight: 1) *age* of the patient, 2) *sex* (0=female, 1=male), 3) *cp* - chest pain type experienced (0=typical angina, 1= atypical angina, 2=non-anginal pain, 3=asyptomatic), 4) *chol* - the patientâ€™s cholesterol measurement in mg/dl, 5) *exang* - exercise induced angina ( 0=no, 1=yes), 6) *oldpeak* - ST depression induced by exercise relative to rest, 7) *ca* - the number of major vessels by fluoroscopy (0-3), 8) *target* - heart state (0=healthy, 1=heart disease). ST depression refers to the depression of the ST wave segment on an electrocardiogram below baseline. ST depression can be used as a predictor of myocaridal ischemia, or lack of blood flow to the heart which can result in a heart attack.

```{R}
install.packages("tidyr")
library(tidyverse)
library(dplyr)
library(ggplot2)
heart = read.csv("heart.csv")
#removing unwanted columns
heart_dat <- heart %>% select(-c("trestbps", "fbs", "restecg", "thalach", "slope", "thal"))

```

#MANOVA
####With a MANOVA, I will simultaneously test whether ST wave depression (oldpeak), cholesterol measurement, and age differ by heart state (healthy (0) vs. heart disease present (1)). The null hypothesis is that for each numeric response variable (oldpeak, chol, and age), there is not a mean difference across target heart state. Examination of the bivariate density plots revealed that the data did not meet multivariate normality. Because there are many assumptions for a MANOVA, it is generally hard to test or meet, and it is likely that many were not met in this situation. Regarldess, the test was run. After running the MANOVA, it was observed that there is in fact a significant difference in ST wave depression, cholesterol levels, and age across target heart state, Pillai trace=0.206, pseudo F (3,299)=25.816, p=7.047e-15 < 0.001. Univariate ANOVAs for each numeric variable were performed. The univariate ANOVA for cholesterol was not significant (p=0.1388). However, the univariate ANOVA for oldpeak was significant (F(1,301)=68.551, p=4.085e-15 < 0.001), and the univariate ANOVA for age was also significant (F(1,301)=16.117,p=7.525e-05 < 0.001). Post hoc analysis was performed by conducting pairwise comparisons to determine which target heart state differed in age, cholesterol, and oldpeak. It revealed that age and ST wave depression differ, but not cholesterol. However, in reality the pot hoc t tests were unecessary since I had one MANOVA with three numeric response variables and a categorical predictor with 2 levels. The univariate ANOVAs indicated which variables were significantly different. Thus,a total of four tests were conducted.Because of this, the probability of at least one type I error is 0.1855. Therefore, the Bonferoni adjustment would be 0.05/4=0.0125. Even with the Bonferroni adjustment, only the ST wave depression (oldpeak) and age remain sigificantly different across target heart state. 
```{R}
#MANOVA
man1 <- manova(cbind(oldpeak,chol, age) ~ target, data = heart_dat)
summary(man1)

#Univariate ANOVAs
summary.aov(man1)

heart_dat %>% group_by(target) %>% summarize(mean(oldpeak), mean(chol), mean(age))

#Posthoc t tests
pairwise.t.test(heart_dat$oldpeak, heart_dat$target, p.adj="none")

pairwise.t.test(heart_dat$chol, heart_dat$target, p.adj="none")

pairwise.t.test(heart_dat$age, heart_dat$target, p.adj="none")

#multivariate normality
ggplot(heart_dat, aes(x = chol, y = age)) +   geom_point(alpha = .8) +  facet_wrap(~target) + ggtitle("Multivariate normaility for age and cholesterol by target")

ggplot(heart_dat, aes(x = chol, y = oldpeak)) +   geom_point(alpha = .8) + facet_wrap(~target) + ggtitle("Multivariate normaility for oldpeak and cholesterol by target")


ggplot(heart_dat, aes(x = age, y = oldpeak)) +   geom_point(alpha = .8) + facet_wrap(~target) + ggtitle("Multivariate normaility for age and oldpeak by target")


```

#Randomization Test
####I wanted to test how average ST depression compared across chest pain type. To test this, I used an ANOVA where my null hypothesis was that average ST depression for each chest pain type group was the same. The alternative hypothesis is that at least one of the mean ST depression differs in one chest pain type group from the other chest pain types. To test the null hypothesis, I calculated the F statistic and looked at the variation between groups versus the variation within groups. A histogram of the data was made, and it appears that the data failed to meet the assumption of normal distribution. The sum of squares between (SSB) and sum of squares within (SSW) were computed manually for the randomization test and then were used to calculate the F statistic. The resulting F statistic was 14.273. For comparison, the F statistic was also calculated from the aov() function, and the resulting F statistic from that method was 14.27. Comparing the means of the two, the resulting p value was 1. Based off the one-way ANOVA using the aov(), we observed that there is in fact a difference in average ST wave depression across chest pain types (p=0.00928 < 0.05).
```{R}

heart_dat$cp<-as.factor(heart_dat$cp)
summary(aov(oldpeak~cp, data=heart_dat))
obs_F<- 14.27

SSW <- heart_dat %>% group_by(cp) %>% summarize(SSW = sum((oldpeak -mean(oldpeak))^2)) %>% summarize(sum(SSW)) %>% pull
SSB <- heart_dat %>% mutate(mean = mean(oldpeak)) %>% group_by(cp) %>% mutate(groupmean = mean(oldpeak)) %>% summarize(SSB = sum((mean -groupmean)^2)) %>% summarize(sum(SSB)) %>% pull 
F_stat <- (SSB/3)/(SSW/299)
F_stat


Fs<-replicate(5000,{
  new<-heart_dat%>%mutate(oldpeak=sample(oldpeak))
  SSW<- new%>%group_by(cp)%>%summarize(SSW=sum((oldpeak-mean(oldpeak))^2))%>%summarize(sum(SSW))%>%pull
  SSB<- new%>%mutate(mean=mean(oldpeak))%>%group_by(cp)%>%mutate(groupmean=mean(age))%>%
    summarize(SSB=sum((mean-groupmean)^2))%>%summarize(sum(SSB))%>%pull
  (SSB/3)/(SSW/299)
})

hist(Fs, prob=T); abline(v = obs_F, col="red",add=T)

mean(Fs > obs_F)


```


#Linear Regression 
####A buildup in cholesterol in arteries can block blood flow and ultimately result in a heart attack. Because of this, I wanted to see how cholesterol levels affect prediction of a heart attack as given by ST depression. I used a linear regression model to predict ST wave depression from cholesterol levels and chest pain type. My null hypothesis is that while controlling for chest pain type, cholesterol does not explain variation in ST wave depression. The other null hypothesis is that while controlling for cholesterol levels, chest pain type does not explain variation in ST wave depression. After running the linear regression and looking at the coefficients, it was observed that while controlling for cholesterol, there is no significant effect of gender on ST wave depression. For every one unit increase in cholesterol, there is at 0.0017mm increase in ST wave depression, t=1.299, df=300,p=0.149. Controlling for gender, there is no significant effect of cholesterol on ST wave depression, t=1.901, df=300, p=0.0583. The interactions between cholesterol levels and gender were studied as well. The slopes for males and females are not signifcantly different (b3=-0.0006, t=-0.238, p=0.8124). The assumptions of linearity, normality, and homoskedaticity are not met as given by graphical representation and the Breuch-Pagan test (p=0.8488). The regression results were recomputed with robust standard error. Even after computing with robust SE, there were no significant changes as all variables and interactions remained insignifcant for both models with and without interaction of cholesterol and sex. There was no significant effect of cholesterol (t=-1.56,p=0.33) or gender(t=1.87, p=0.06),or their interactions(t=-0.21, p=0.83) on ST wave depression. Based off the calculated R^2, the model explains 81.9% of the variation in the outcome. 
```{R}
#linear regression without interaction
fit <- lm(oldpeak ~ chol + sex, data=heart_dat)
summary(fit)

#linear regression with interaction 
fit2 <- lm(oldpeak ~ chol * sex, data=heart_dat)
summary(fit2)


#linear regression without interaction after mean-centering cholesterol and ST wave depression
heart_dat$chol_c <- heart_dat$chol - mean(heart_dat$chol,na.rm=T)
heart_dat$oldpeak_c <- heart_dat$oldpeak - mean(heart_dat$oldpeak, na.rm=T)
heart_dat$sex<-as.factor(heart_dat$sex)
fit3 <- lm(oldpeak_c ~ chol_c + sex, data=heart_dat)
summary(fit3)

#linear regression with interaction after mean-centering cholesterol and ST wave depression
fit4 <- lm(oldpeak_c ~ chol_c * sex, data=heart_dat)
summary(fit4)

#Plotting the regression without interaction
heartdata<-heart
heartdata$chol_c <- heartdata$chol - mean(heartdata$chol,na.rm=T)
heartdata$oldpeak_c <- heartdata$oldpeak - mean(heartdata$oldpeak, na.rm=T)

newdat<-heart_dat
newdat$sex <- as.numeric(newdat$sex)
fit5<-lm(oldpeak_c~chol_c+sex, data=heartdata)
newdat$pred2<-predict(fit5,newdat)
newdat$pred1<-predict(fit5,newdat)
new1<-newdat%>%select(chol,oldpeak,pred2,pred1)%>%gather(sex,pred2,pred1,-c(chol,oldpeak))
new1$sex<-factor(newdat$sex,labels=c("Male","Female"))
ggplot(new1,aes(x=chol,y=oldpeak))+geom_point()+ geom_smooth(method="lm",formula=y~1,se=F,fullrange=T,aes(color=sex))+  theme(legend.position=c(.8,.8))+ggtitle("Graph of multiple regression") 

#Plotting the regression with interaction
heartdata<-heart
heartdata$chol_c <- heartdata$chol - mean(heartdata$chol,na.rm=T)
heartdata$oldpeak_c <- heartdata$oldpeak - mean(heartdata$oldpeak, na.rm=T)

newdat<-heart_dat
newdat$sex <- as.numeric(newdat$sex)
fit6<-lm(oldpeak_c~chol_c*sex, data=heartdata)
newdat$pred2<-predict(fit6,newdat)
newdat$pred1<-predict(fit6,newdat)
new1<-newdat%>%select(chol,oldpeak,pred2,pred1)%>%gather(sex,pred2,pred1,-c(chol,oldpeak))
new1$sex<-factor(newdat$sex,labels=c("Male","Female"))
ggplot(new1,aes(x=chol,y=oldpeak))+geom_point()+ geom_smooth(method="lm",formula=y~1,se=F,fullrange=T,aes(color=sex))+  theme(legend.position=c(.8,.8))+ggtitle("Graph of multiple regression") 

#checking assumptions of linearity, normality, and homoskedacity (for fit without interaction)
resids<- fit5$residuals
fitvals<-fit5$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red') + ggtitle("Graph of residuals vs fitted values without interaction")

library(lmtest) 
library(sandwich)
bptest(fit5)

ggplot() + geom_histogram(aes(resids), bins=20) + ggtitle("Histogram of residuals")

ggplot() +geom_qq(aes(sample=resids))+ geom_qq_line(aes(sample=resids)) + ggtitle("Q-Q plot")


#checking assumptions of linearity, normality, and homoskedacity (for fit with interaction)
resids2<- fit6$residuals
fitvals2<-fit6$fitted.values
ggplot()+geom_point(aes(fitvals2,resids2))+geom_hline(yintercept=0, color='red')+ ggtitle("Graph of residuals vs fitted values with interaction")

library(lmtest) 
library(sandwich)
bptest(fit6)

ggplot() + geom_histogram(aes(resids2), bins=20) + ggtitle("Histogram of residuals with interaction")

ggplot() +geom_qq(aes(sample=resids2))+ geom_qq_line(aes(sample=resids2)) + ggtitle("Q-Q plot with interaction")

#Recomputing with robust standard errors (for fit without interaction)
coeftest(fit5, vcov = vcovHC(fit5))


#Recomputing with robust standard errors (for fit with interaction)
coeftest(fit6, vcov = vcovHC(fit6))

#Proportion of variation
sum((fitvals-mean(heart_dat$oldpeak))^2)/sum((heart_dat$oldpeak-mean(heart_dat$oldpeak))^2)



```



#Linear Regression with bootstrapped strapped standard errors
##I reran the same regression model with interaction but with bootstrapped standard errors. I also ran the bootstrapped residual standard errors for comparison.Overall the bootstrapped standard errors gave more conservative (larger) estimates than the residual standard errors, so I decided to go with the bootstrapped standard errors. The standard errors from the original standard errors, the robust standard errors, and the bootstrapped standard errors are all similar in value across all variables. For example for cholesterol, the SE is 0.0018, the robust SE is 0.0021, and the bootstrapped SE is 0.0022. Overall, they are all fairly similar although they slightly increase across different methods of calculating SE. The p-values for all variables across methods remain not signficant. 
```{R}
#Bootstrapped Standard Error 
boot_dat<-heart_dat[sample(nrow(heart_dat),replace=TRUE),]

samp_distn<-replicate(5000, {   
  boot_dat<-heart_dat[sample(nrow(heart_dat),replace=TRUE),]   
  fit<-lm(oldpeak_c~chol_c*sex,data=boot_dat)   
coef(fit) })

##Estimated SEs
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)


#comparison to standard error and robust standard error for model with interaction from above
fit6<-lm(oldpeak_c~chol_c*sex, data=heartdata)
coeftest(fit6)
coeftest(fit6, vcov = vcovHC(fit6))


```


#Logistic Regression
##I decided to use a logistic regression to predict either a healthy heart or heart disease state based off two explanatory variables - ST wave depression and chest pain type. The heart state is under the variable 'target' where 0=healthy and 1=heart disease. Controlling for chest pain type, a decrease in ST wave depression increases chances of not having heart disease (p=3.56e-08<0.05). Compared to chest pain type 0, all other chest pain types have a higher probability of the patient having heart disease. Next, the odds were observed by exponentiating all of the coefficientts. The odds of a patient having heart disease with an ST wave depression of 0 mm and a chest pain type of 0 is 0.97. Every 1mm increase in ST wave depression, increases odds of heart disase by a factor of 0.4. Compared to chest pain (CP) type 0, odds for CP1 are 6.6 times higher, odds for CP2 are 9.57 times higehr, and odds for CP 3 are 9.24 times higher.  The accuracy was computed to be 0.768. The probability of predicting if a patient has heart disease if they really have it, otherwise known as the sensitivity or true positive rate, is 0.815. The specificity, or the true negative rate, was calculated to be 0.724. The precision (PPV), or the proportion of those classified as having heart disease who actually do have it, was calculated to be 0.745. Next, the ROC plot was made and the AUC was calculated to be 0.8417. This indicates that the model does a fairly good job at predicting if  a patient has heart disease. The defined cutoffs for a good AUC is 0.8 to 0.9. Of course, there is still room for improvement; making the auc higher would be ideal, especially since the false negative rate is 0.276. In a clinical setting, this false negative rate for predicting heart disease could have serious impacts and risks because if a patient was sent home on the premise that they were healthy, their life could be endangered. Next, the model was tested under a 10-fold cross validation. Under the 10-fold CV, the model was fairly good at predicting heart disease. The cross-validation AUC was 0.826, which is only 0.0157 lower than the original AUC. This means that the model did not way overfit the data. After CV, the average out of sample accuracy was 0.756, sensitivity was 0.847, specificity was 0.643, and PPV was 0.745. 
```{R}
#Logistic regresssion
heart_dat$cp <- factor(heart_dat$cp)
logfit <- glm(target ~ oldpeak_c + cp, data=heart_dat, family="binomial")
coeftest(logfit)
exp(coeftest(logfit))

#Confusion matrix
prob<-predict(logfit, type="response")
pred <- ifelse(prob>0.5, 1, 0)
table(truth=heart_dat$target, prediction=pred)%>% addmargins

#Computing accuracy, sensitivity, and specificity 
#accuracy
(110+123)/303

#sensitivity (TPR)
123/151

#specificity (TNR)
110/152

#precision (PPV)
123/165


#Plot density of log odds (logit) by target outcome variable
heart_dat$logit<-predict(logfit,type="link")
heart_dat$target <- as.factor(heart_dat$target)
heart_dat%>%ggplot()+geom_density(aes(logit,color=target,fill=target), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("predictor (logit)")+ggtitle("Density of log odds graph")


#Generating ROC curve 
library(plotROC)
heart_dat <- heart %>% select(-c("trestbps", "fbs", "restecg", "thalach", "slope", "thal"))
ROCplot <- ggplot(heart_dat) + geom_roc(aes(d=target,m=prob), n.cuts=0)+ geom_segment(aes(x=0,xend=1,y=0,yend=1), lty=2) + ggtitle("ROC plot")
ROCplot

calc_auc(ROCplot)

#10-fold Cross Validation 
#creating a smaller data frame to only include the variables of interest for the model
library(boot)
heartdat_cv <- heart_dat%>% select(c(target, cp, oldpeak))
heartdat_cv<-na.omit(heartdat_cv)
heartdat_cv$cp <- as.numeric(heartdat_cv$cp)
heartdat_cv$oldpeak <- as.numeric(heartdat_cv$oldpeak)
heartdat_cv$target <- as.numeric(heartdat_cv$target)

fit_cv<-glm(target~cp+oldpeak, data=heartdat_cv, family="binomial")
summary(fit_cv)
prob_cv <- predict(fit_cv, type="response")
pred_cv<- ifelse(prob>0.5, 1, 0)
rbind(prob_cv, pred_cv, truth=heartdat_cv$target)[,1:10]%>% round(3)

table(predictions=pred_cv, truth=heartdat_cv$target)

heartdat_cv$prob_cv<-predict(fit_cv,type="response")

#Creating function for class diagnostics
class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #calculating exact AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  
  data.frame(acc,sens,spec,ppv,auc)
}


variable <- class_diag(heartdat_cv$prob_cv, heartdat_cv$target)
variable


#10-fold Cross validation 
set.seed(1234)
k=10

data1 <- heartdat_cv[sample(nrow(heartdat_cv)),]
folds <- cut(seq(1:nrow(heartdat_cv)), breaks=k, labels=F)

diags<-NULL
for(i in 1:k){ 
  train<-data1[folds!=i,] 
  test<-data1[folds==i,] 
  truth<-test$target
  
  fit_cv<- glm(target~., data=train, family="binomial")
    probs_cv<- predict(fit_cv, newdata=test, type="response")
      
      diags<-rbind(diags,class_diag(probs_cv,truth)) 
}
apply(diags,2,mean)


```


#LASSO regression
##Using the same binary prediction variable as above (target heart state) a LASSO regression was run. The regression was run setting lambda equal to cv$lamdba.1se. The results revealed that age, sex, chest pain type, exercise-induced angina, ST wave depression, number of major vessels (ca) and cholesterol levels are the most important predictors of having a healthy heart or having heart disease. A 10-fold CV was run using this model. From this model, the accuracy and AUC were both higher than those from the logistic regression performed earlier in part 5. From the LASSO regression model, the accuracy was 0.802 and the AUC was 0.886. From the earlier logistic regression model, the accuracy was 0.756 and the AUC was 0.826. The LASSO regression model brings the AUC to be closer to the "great" range, whic is defined as an AUC between 0.9-1.0. 
```{R}
#LASSO regularization on a logistic regression (data has categorical variables)
library(glmnet)
heartdat_lasso <- heart_dat

#turning some variables into factor because they are categorical variables
heartdat_lasso$sex <- as.factor(heartdat_lasso$sex)
heartdat_lasso$exang <- as.factor(heartdat_lasso$exang)
heartdat_lasso$cp <- as.factor(heartdat_lasso$cp)



fit_lasso <- glm(target ~ -1+ age + sex + cp +exang+oldpeak+ca+chol, data = heartdat_lasso, family = "binomial") 
head(model.matrix(fit))

set.seed(1234) 
x<-model.matrix(fit_lasso) 

x<-scale(x) 

y<-as.matrix(heart_dat$target)

cv2<-cv.glmnet(x,y,family="binomial") 

lasso2<-glmnet(x,y,family="binomial",lambda=cv2$lambda.1se) 
coef(cv2)


k=10 

data1<-heartdat_lasso[sample(nrow(heartdat_lasso)),] 
folds<-cut(seq(1:nrow(heartdat_lasso)),breaks=k,labels=F)

diags<-NULL 
for(i in 1:k){   
  train<-data1[folds!=i,]   
  test<-data1[folds==i,]   
  truth<-test$target
  
  fit<-glm(target~.,data=train,family="binomial")   
  probs<-predict(fit,newdata = test,type="response")

  diags<-rbind(diags,class_diag(probs,truth)) }

diags%>%summarize_all(mean)


```


